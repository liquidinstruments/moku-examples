{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "WIMi9Ux13rjn",
        "feVsynnk3hW5",
        "LklG_HOr3HGT"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Loading a LINN Model and Continuing Training\n",
        "\n",
        "\n",
        "This notebook introduces the new `load_linn` function.\n",
        "\n",
        "**Why this is useful:**  \n",
        "Real-world performance can drift after deployment. Being able to **load an existing `.linn` model** and **continue training** lets you quickly adapt without rebuilding from scratch.\n",
        "\n",
        "Common scenarios:\n",
        "- **New data arriving** (e.g. from running sensors).\n",
        "- **Deployed performance not meeting expectations** once on-device (e.g. noise profiles, temperature, latency constraints).\n",
        "- **Distribution shift / drift** over time (e.g. sensor aging, calibration changes, firmware updates).\n",
        "\n",
        "The tutorial below demonstrates how to **load an existing `.linn` model** and then **fine-tune it with additional data**."
      ],
      "metadata": {
        "id": "_2If4k9d3NY8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Just Importing the Class"
      ],
      "metadata": {
        "id": "WIMi9Ux13rjn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMwNx7YCygm5"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import sys\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "import logging\n",
        "from typing import Any, Optional, List, Tuple\n",
        "from copy import copy, deepcopy\n",
        "\n",
        "# list of supported activations\n",
        "_AVAILABLE_ACTIVATIONS = [\"relu\", \"tanh\", \"sigmoid\", \"softsign\", \"linear\"]\n",
        "_LOG = logging.getLogger(\"Linn\")\n",
        "\n",
        "\n",
        "def list_activations():\n",
        "    \"\"\"\n",
        "    List the available activation functions\n",
        "    :return: List of available activation functions\n",
        "    \"\"\"\n",
        "    return _AVAILABLE_ACTIVATIONS\n",
        "\n",
        "\n",
        "class WeightClip(keras.constraints.Constraint):\n",
        "\n",
        "    def __init__(self, clip_value: float = 1.0) -> None:\n",
        "        \"\"\"\n",
        "        Clips all the weights/biases to be within the range [-1, 1] to help with quantization later on.\n",
        "        :param clip_value: value to clip the model between (float).\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.clip_value = float(clip_value)\n",
        "\n",
        "    def __call__(self, w: Any):\n",
        "        \"\"\"\n",
        "        Clip the weights/biases to the symmetric bounds\n",
        "        :param w: Tensor or variable representing the weights\n",
        "        :return: Tensor or variable clipped to the bounds\n",
        "        \"\"\"\n",
        "        return tf.clip_by_value(w, -self.clip_value, self.clip_value)\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\"name\": self.__class__.__name__, \"clip_value\": self.clip_value}\n",
        "\n",
        "\n",
        "class OutputClipLayer(keras.layers.Layer):\n",
        "    def __init__(self, clip_value: float = 1.0):\n",
        "        \"\"\"\n",
        "        Class for clipping the output layers of the network. Ensures that outputs are clipped to [-1, 1] bounds for\n",
        "        quantization.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.clip_value = float(clip_value)\n",
        "\n",
        "    def call(self, inputs, *args, **kwargs):\n",
        "        \"\"\"\n",
        "        Clip the inputs to the symmetric bounds\n",
        "        :param inputs: The output of the previous layer\n",
        "        :param args: Mirrored from base layer class.\n",
        "        :param kwargs: Mirrored from base layer class.\n",
        "        :return: Tensor or variable clipped to the bounds\n",
        "        \"\"\"\n",
        "        return tf.clip_by_value(inputs, -self.clip_value, self.clip_value)\n",
        "\n",
        "\n",
        "class LinnModel:\n",
        "    def __init__(self):\n",
        "        self.model = None\n",
        "        self.training_inputs = None\n",
        "        self.training_outputs = None\n",
        "\n",
        "        self._input_transform_args = None\n",
        "        self._output_transform_args = None\n",
        "\n",
        "        self.model_definition = None\n",
        "\n",
        "    def construct_model(\n",
        "        self,\n",
        "        layer_definition: list = None,\n",
        "        show_summary: bool = False,\n",
        "        optimizer: any = \"adam\",\n",
        "        loss: any = \"mse\",\n",
        "        metrics: any = (),\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Construct the model to be used by the rest of the functions in this class.\n",
        "        :param layer_definition: a list of tuples of the form `(layer_width, activation)`\n",
        "        which defines the model. If not provided the default model will be used.\n",
        "        `(layer_width,)` can be used to signify a linear (identity) activation function.\n",
        "        :param show_summary: (bool) determines whether the model summary is displayed\n",
        "        :param optimizer: optimizer fed to the keras compile option.\n",
        "        :param loss: loss function fed to the keras compile option.\n",
        "        :param metrics: metrics for the model to track during training\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "\n",
        "        # check the input and output\n",
        "        if self.training_inputs is None or self.training_outputs is None:\n",
        "            raise TypeError(\"Please set the training data first.\")\n",
        "\n",
        "        # check the model definition type\n",
        "        if layer_definition is not None:\n",
        "            if type(layer_definition) is not list:\n",
        "                raise TypeError(\n",
        "                    f\"Expected type:<list> for model definition. Received: <{type(layer_definition)}>.\"\n",
        "                )\n",
        "            elif len(layer_definition) == 0:\n",
        "                raise ValueError(\"layer_definition can't be empty.\")\n",
        "        else:\n",
        "            raise ValueError(\"layer_definition can't be empty.\")\n",
        "\n",
        "        # assign the definition for checking\n",
        "        self.model_definition = layer_definition\n",
        "\n",
        "        # construct the model\n",
        "        input_layer = keras.layers.Input((self.training_inputs.shape[-1],))\n",
        "        prev_layer = input_layer\n",
        "        # prev_layer = None\n",
        "        for idx, layer_defn in enumerate(layer_definition):\n",
        "            if type(layer_defn) is not tuple:\n",
        "                raise TypeError(\n",
        "                    f\"layer_definition index:{idx} is type:<{type(layer_defn)} not <tuple>.\"\n",
        "                )\n",
        "\n",
        "            if len(layer_defn) != 2:\n",
        "                raise ValueError(\n",
        "                    f\"layer_definition index:{idx} should have length 2, not {len(layer_defn)}.\"\n",
        "                )\n",
        "\n",
        "            # grab the activation function\n",
        "            # activation = 'linear'\n",
        "\n",
        "            activation = str(layer_defn[1])\n",
        "            if activation not in list_activations():\n",
        "                raise ValueError(\n",
        "                    f\"Activation {activation} is not in the list of supported activations. \"\n",
        "                    f\"Try {list_activations()}.\"\n",
        "                )\n",
        "            # grab the layer width\n",
        "            try:\n",
        "                layer_width = int(layer_defn[0])\n",
        "            except ValueError as e:\n",
        "                raise ValueError(f\"Error converting the layer width: {e.args}.\")\n",
        "\n",
        "            prev_layer = keras.layers.Dense(\n",
        "                layer_width,\n",
        "                activation=activation,\n",
        "                kernel_constraint=WeightClip(),\n",
        "                bias_constraint=WeightClip(),\n",
        "            )(prev_layer)\n",
        "            prev_layer = OutputClipLayer()(prev_layer)\n",
        "\n",
        "        # compile the model for training\n",
        "        self.model = keras.Model(input_layer, prev_layer)\n",
        "        self.model.compile(\n",
        "            optimizer=optimizer, loss=loss, metrics=[met for met in metrics]\n",
        "        )\n",
        "\n",
        "        # summarize if necessary\n",
        "        if show_summary:\n",
        "            self.model.summary()\n",
        "\n",
        "\n",
        "    def _check_model_definition(self):\n",
        "        # Check the input_layer first\n",
        "        if type(self.model_definition[0]) is not tuple:\n",
        "            _LOG.warning(\n",
        "                f\"Definition of input_layer is type:<{type(self.model_definition[0])} not <tuple>.\"\n",
        "            )\n",
        "\n",
        "        if len(self.model_definition[0]) != 1:\n",
        "            _LOG.warning(\n",
        "                f\"Definition of input_layer should have length 1, not {len(self.model_definition[0])}.\"\n",
        "            )\n",
        "\n",
        "        if len(self.model_definition[0]) == 2:\n",
        "            _LOG.warning(f\"Ignored the activation function in the input_layer\")\n",
        "\n",
        "        if self.model_definition[0][0] != self.training_inputs.shape[-1]:\n",
        "            _LOG.warning(\n",
        "                f\"Shape of layer_definition index: 0 not match the shape of input data, should be \"\n",
        "                f\"{self.training_inputs.shape[-1]}, not {self.model_definition[0][0]}. Changed to the shape of\"\n",
        "                f\" input data automatically.\"\n",
        "            )\n",
        "\n",
        "        self.model_definition[0] = (self.training_inputs.shape[-1],)\n",
        "\n",
        "        for idx, layer_defn in enumerate(self.model_definition[1:]):\n",
        "            if type(layer_defn) is not tuple:\n",
        "                raise TypeError(\n",
        "                    f\"layer_definition index:{idx+1} is type:<{type(layer_defn)} not <tuple>.\"\n",
        "                )\n",
        "\n",
        "            if len(layer_defn) not in [1, 2]:\n",
        "                raise ValueError(\n",
        "                    f\"layer_definition index:{idx+1} should have length 1 or 2, not {len(layer_defn)}.\"\n",
        "                )\n",
        "\n",
        "            if len(layer_defn) == 2:\n",
        "                if layer_defn[1] not in list_activations():\n",
        "                    raise ValueError(\n",
        "                        f\"Activation {layer_defn[1]} is not in the list of supported activations. \"\n",
        "                        f\"Try {list_activations()}.\"\n",
        "                    )\n",
        "\n",
        "        if self.model_definition[-1][0] != self.training_outputs.shape[-1]:\n",
        "            _LOG.warning(\n",
        "                f\"Shape of layer_definition index: {len(self.model_definition)-1} does not match the shape of \"\n",
        "                f\"output data, should be {self.training_outputs.shape[1:]}, not \"\n",
        "                f\"{self.model_definition[-1][0]}. Changed to the shape of output data automatically\"\n",
        "            )\n",
        "            if len(self.model_definition[-1]) == 2:\n",
        "                self.model_definition[-1] = (\n",
        "                    self.training_outputs.shape[-1],\n",
        "                    self.model_definition[-1][1],\n",
        "                )\n",
        "            else:\n",
        "                self.model_definition[-1] = (self.training_outputs.shape[-1],)\n",
        "\n",
        "        return True\n",
        "\n",
        "    def _check_data_model_dim(\n",
        "        self, inputs: np.ndarray, outputs: np.ndarray, data_name: str\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Check the data and model dimensions against one another to make sure they are correct.\n",
        "        :param inputs: Inputs to the model\n",
        "        :param outputs: Outputs of the model\n",
        "        :param data_name: String indicating whether it's training or validation.\n",
        "        :return: Boolean indicating whether everything is ok\n",
        "        \"\"\"\n",
        "        # warn the user dimension checking is skipped\n",
        "        if self.model is None:\n",
        "            _LOG.warning(\n",
        "                \"Keras model is not initialized. Dimension checking will be skipped.\"\n",
        "            )\n",
        "            return False\n",
        "\n",
        "        # check if the dimensions match the defined model\n",
        "        if (\n",
        "            len(inputs.shape) != len(self.model.input_shape)\n",
        "            or inputs.shape[1:] != self.model.input_shape[1:]\n",
        "        ):\n",
        "            raise ValueError(\n",
        "                f\"Dimensions of {data_name} inputs do not match model: {inputs.shape[1:]}, \"\n",
        "                f\"{self.model.input_shape[1:]}\"\n",
        "            )\n",
        "\n",
        "        if (\n",
        "            len(outputs.shape) != len(self.model.output_shape)\n",
        "            or outputs.shape[1:] != self.model.output_shape[1:]\n",
        "        ):\n",
        "            raise ValueError(\n",
        "                f\"Dimensions of {data_name} outputs do not match model: {outputs.shape[1:]}, \"\n",
        "                f\"{self.model.output_shape[1:]}\"\n",
        "            )\n",
        "\n",
        "        # if we got this far, success!\n",
        "        return True\n",
        "\n",
        "    def _transform(self, data_array: np.ndarray, is_input: bool):\n",
        "        \"\"\"\n",
        "        Transform the data into a domain amenable for quantization.\n",
        "        :param data_array: Data array to be transformed\n",
        "        :param is_input: is it an input or output?\n",
        "        :return: Return the transformed array\n",
        "        \"\"\"\n",
        "        # copy the array so we don't modify the original\n",
        "        output_array = np.copy(data_array)\n",
        "        # get the scales if they are missing\n",
        "        if is_input and self._input_transform_args is None:\n",
        "            self._input_transform_args = (\n",
        "                np.min(output_array, axis=0),\n",
        "                np.max(output_array, axis=0),\n",
        "            )\n",
        "        if not is_input and self._output_transform_args is None:\n",
        "            self._output_transform_args = (\n",
        "                np.min(output_array, axis=0),\n",
        "                np.max(output_array, axis=0),\n",
        "            )\n",
        "\n",
        "        # scale everything according to the transform arguments\n",
        "        if is_input:\n",
        "            output_array -= self._input_transform_args[0]\n",
        "            output_array /= (\n",
        "                self._input_transform_args[1] - self._input_transform_args[0]\n",
        "            )\n",
        "        else:\n",
        "            output_array -= self._output_transform_args[0]\n",
        "            output_array /= (\n",
        "                self._output_transform_args[1] - self._output_transform_args[0]\n",
        "            )\n",
        "        output_array = 2 * (output_array - 0.5)\n",
        "\n",
        "        return output_array\n",
        "\n",
        "    def _inverse_transform(self, data_array: np.ndarray, is_input: bool):\n",
        "        \"\"\"\n",
        "        Undo the scaling of the transform function to bring data back to the original domain.\n",
        "        :param data_array: Data to be transformed\n",
        "        :param is_input: is it the input or output\n",
        "        :return: Return the transformed array\n",
        "        \"\"\"\n",
        "        # copy the array so we don't destroy the original\n",
        "        output_array = np.copy(data_array)\n",
        "        # see if things can be transformed\n",
        "        if is_input and self._input_transform_args is None:\n",
        "            raise RuntimeError(\n",
        "                \"Cannot inverse transform before first calling transform.\"\n",
        "            )\n",
        "        if not is_input and self._output_transform_args is None:\n",
        "            raise RuntimeError(\n",
        "                \"Cannot inverse transform before first calling transform.\"\n",
        "            )\n",
        "\n",
        "        # perform the inverse scaling\n",
        "        output_array /= 2.0\n",
        "        output_array += 0.5\n",
        "        if is_input:\n",
        "            output_array *= (\n",
        "                self._input_transform_args[1] - self._input_transform_args[0]\n",
        "            )\n",
        "            output_array += self._input_transform_args[0]\n",
        "        else:\n",
        "            output_array *= (\n",
        "                self._output_transform_args[1] - self._output_transform_args[0]\n",
        "            )\n",
        "            output_array += self._output_transform_args[0]\n",
        "\n",
        "        return output_array\n",
        "\n",
        "    def set_training_data(\n",
        "        self,\n",
        "        training_inputs: np.ndarray,\n",
        "        training_outputs: np.ndarray,\n",
        "        scale: bool = True,\n",
        "        input_data_boundary: Tuple[list] = None,\n",
        "        output_data_boundary: Tuple[list] = None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Set the training data to be used by the model and automatically scale them to use the full dynamic range if\n",
        "        specified. Scaling will ensure the model inputs and outputs are in the domain [-1, 1]. Call this before\n",
        "        constructing a model to assign the data dimensions in construction.\n",
        "        :param training_inputs: numpy.ndarray of input training data which should match the model dimensions\n",
        "        :param training_outputs: numpy.ndarray of output training data which should match the model dimensions\n",
        "        :param scale: (bool) automatically scale the data to fit in the bounds.\n",
        "        :param input_data_boundary: tuple of the input data boundaries that should be used for scaling\n",
        "        :param output_data_boundary: tuple of the output data boundaries that should be used for scaling\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "\n",
        "        # We copy the input data so the scaling doesn't affect the original. This can be big, so might\n",
        "        # want to revisit and e.g. only copy if we're scaling\n",
        "        training_inputs = np.array(copy(training_inputs), dtype=float)\n",
        "        training_outputs = np.array(copy(training_outputs), dtype=float)\n",
        "\n",
        "        if not scale and (\n",
        "            input_data_boundary is not None or output_data_boundary is not None\n",
        "        ):\n",
        "            _LOG.warning(f\"scale = False. Ignoring the boundary setting\")\n",
        "\n",
        "        if scale:\n",
        "            if input_data_boundary is not None:\n",
        "                if (\n",
        "                    not isinstance(input_data_boundary, (list, tuple))\n",
        "                    or len(input_data_boundary) != 2\n",
        "                ):\n",
        "                    raise TypeError(\n",
        "                        \"Expected tuple of the form (lower_bound, upper_bound) for boundary setting.\"\n",
        "                    )\n",
        "\n",
        "                if not isinstance(input_data_boundary[0], (np.ndarray, list)):\n",
        "                    raise TypeError(\n",
        "                        f\"Lower bounds should be type:<numpy.ndarray> or type:<list> not {type(input_data_boundary[0])}.\"\n",
        "                    )\n",
        "\n",
        "                if not isinstance(input_data_boundary[1], (np.ndarray, list)):\n",
        "                    raise TypeError(\n",
        "                        f\"Upper bounds should be type:<numpy.ndarray> or type:<list> not {type(input_data_boundary[1])}.\"\n",
        "                    )\n",
        "\n",
        "                if len(input_data_boundary[0]) != training_inputs.shape[-1]:\n",
        "                    raise ValueError(\n",
        "                        f\"Dimensions of lower bounds do not match input: {len(input_data_boundary[0])}, \"\n",
        "                        f\"{training_inputs.shape[-1]}\"\n",
        "                    )\n",
        "                if len(input_data_boundary[1]) != training_inputs.shape[-1]:\n",
        "                    raise ValueError(\n",
        "                        f\"Dimensions of upper bounds do not match input: {len(input_data_boundary[1])}, \"\n",
        "                        f\"{training_inputs.shape[-1]}\"\n",
        "                    )\n",
        "\n",
        "            if output_data_boundary is not None:\n",
        "                if (\n",
        "                    not isinstance(output_data_boundary, (list, tuple))\n",
        "                    or len(output_data_boundary) != 2\n",
        "                ):\n",
        "                    raise TypeError(\n",
        "                        \"Expected tuple of the form (lower_bound, upper_bound) for boundary setting.\"\n",
        "                    )\n",
        "\n",
        "                if not isinstance(output_data_boundary[0], (np.ndarray, list)):\n",
        "                    raise TypeError(\n",
        "                        f\"Lower bounds should be type:<numpy.ndarray> or type:<list> not {type(output_data_boundary[0])}.\"\n",
        "                    )\n",
        "\n",
        "                if not isinstance(output_data_boundary[1], (np.ndarray, list)):\n",
        "                    raise TypeError(\n",
        "                        f\"Upper bounds should be type:<numpy.ndarray> or type:<list> not {type(output_data_boundary[1])}.\"\n",
        "                    )\n",
        "\n",
        "                if len(output_data_boundary[0]) != training_outputs.shape[-1]:\n",
        "                    raise ValueError(\n",
        "                        f\"Dimensions of lower bounds do not match outputs: {len(input_data_boundary[0])}, \"\n",
        "                        f\"{training_outputs.shape[-1]}\"\n",
        "                    )\n",
        "                if len(output_data_boundary[1]) != training_outputs.shape[-1]:\n",
        "                    raise ValueError(\n",
        "                        f\"Dimensions of upper bounds do not match outputs: {len(output_data_boundary[1])}, \"\n",
        "                        f\"{training_outputs.shape[-1]}\"\n",
        "                    )\n",
        "\n",
        "        if input_data_boundary is not None:\n",
        "            self._input_transform_args = [\n",
        "                np.array(boundary) for boundary in input_data_boundary\n",
        "            ]\n",
        "        if output_data_boundary is not None:\n",
        "            self._output_transform_args = [\n",
        "                np.array(boundary) for boundary in output_data_boundary\n",
        "            ]\n",
        "\n",
        "        # scale the data if necessary to scale the full dynamic range\n",
        "        if scale:\n",
        "            training_inputs = self._transform(training_inputs, True)\n",
        "            training_outputs = self._transform(training_outputs, False)\n",
        "\n",
        "        # set the internal variables\n",
        "        self.training_inputs = training_inputs\n",
        "        self.training_outputs = training_outputs\n",
        "\n",
        "    @staticmethod\n",
        "    def _log_missing_value(key: str, default_value: Any, config: dict):\n",
        "        \"\"\"\n",
        "        Convenience function for informing the user they have failed to provide a value and the defaul will be used.\n",
        "        :param key: Key associated with the lookup.\n",
        "        :param default_value: default value that is being used.\n",
        "        :param config: dictionary to be read from\n",
        "        :return: value corresponding to the default or dictionary value\n",
        "        \"\"\"\n",
        "        dict_value = config.get(key, None)\n",
        "        if dict_value is None:\n",
        "            _LOG.warning(f\"Value for {key} missing. Using default:{default_value}.\")\n",
        "            return default_value\n",
        "        else:\n",
        "            return dict_value\n",
        "\n",
        "    def fit_model(\n",
        "        self,\n",
        "        epochs: int,\n",
        "        es_config: Optional[dict] = None,\n",
        "        validation_split: float = 0.0,\n",
        "        validation_data: Optional[tuple] = None,\n",
        "        **keras_kwargs: Optional[Any],\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Fit the model according to the data that is stored in the training inputs and outputs.\n",
        "        set_training_data and construct_model must be called prior to calling Model.fit_model()\n",
        "        :param epochs: (int) number of epochs to train for\n",
        "        :param es_config: configuration dictionary for the early stopping callback\n",
        "        :param validation_split: (float) used to define the validation split\n",
        "        :param validation_data: validation data in a tuple of form (inputs, outputs)\n",
        "        :param keras_kwargs: keyword args to pass to the keras `fit` function\n",
        "        :return: history object from the keras `fit` function\n",
        "        \"\"\"\n",
        "\n",
        "        # cast to an integer or throw an error\n",
        "        epochs = int(epochs)\n",
        "\n",
        "        validation_split = float(validation_split)\n",
        "        if validation_split < 0.0 or validation_split > 1.0:\n",
        "            _LOG.warning(\"Validation split outside of bounds - coercing value.\")\n",
        "            validation_split = np.clip(validation_split, 0.0, 1.0)\n",
        "\n",
        "        # check the validation data\n",
        "        if validation_data is not None:\n",
        "            if type(validation_data) is not tuple or len(validation_data) != 2:\n",
        "                raise TypeError(\n",
        "                    \"Expected tuple of the form (val_input, val_output) for validation data.\"\n",
        "                )\n",
        "\n",
        "            if type(validation_data[0]) is not np.ndarray:\n",
        "                raise TypeError(\n",
        "                    f\"Validation input should be type:<numpy.ndarray> not <{type(validation_data[0])}>.\"\n",
        "                )\n",
        "\n",
        "            if type(validation_data[1]) is not np.ndarray:\n",
        "                raise TypeError(\n",
        "                    f\"Validation output should be type:<numpy.ndarray> not <{type(validation_data[1])}>.\"\n",
        "                )\n",
        "\n",
        "            # check the data dimensions\n",
        "            success = self._check_data_model_dim(\n",
        "                validation_data[0], validation_data[1], \"validation\"\n",
        "            )\n",
        "            if not success:\n",
        "                raise RuntimeError(\n",
        "                    f\"Model is not initialized. Call {self.__class__.__name__}.construct_model to create\"\n",
        "                    f\" the model.\"\n",
        "                )\n",
        "\n",
        "        # warn of a redundancy\n",
        "        if validation_split > 0.0 and (validation_data is not None):\n",
        "            _LOG.warning(f\"Supplied validation_data overrides validation_split.\")\n",
        "\n",
        "        # callbacks to use\n",
        "        callbacks = []\n",
        "\n",
        "        # build the early stopping call back\n",
        "        if es_config is not None:\n",
        "            # make sure it's a dictionary\n",
        "            if type(es_config) is not dict:\n",
        "                raise TypeError(\n",
        "                    f\"Expected type:<dict> for early stopping configuration, not <{type(es_config)}>.\"\n",
        "                )\n",
        "\n",
        "            # parameters for the callback\n",
        "            monitor = self._log_missing_value(\"monitor\", \"val_loss\", es_config)\n",
        "            patience = self._log_missing_value(\"patience\", 5, es_config)\n",
        "            restore = self._log_missing_value(\"restore\", False, es_config)\n",
        "\n",
        "            # callback to be used later\n",
        "            es_callback = keras.callbacks.EarlyStopping(\n",
        "                monitor=monitor,\n",
        "                min_delta=0,\n",
        "                patience=patience,\n",
        "                verbose=0,\n",
        "                mode=\"auto\",\n",
        "                baseline=None,\n",
        "                restore_best_weights=restore,\n",
        "            )\n",
        "            callbacks.append(es_callback)\n",
        "\n",
        "            # fail if there is no validation, yet we monitor it.\n",
        "            if (\n",
        "                validation_split == 0.0\n",
        "                and validation_data is None\n",
        "                and monitor == \"val_loss\"\n",
        "            ):\n",
        "                raise RuntimeError(\n",
        "                    \"Cannot monitor val_loss for early stopping with val_split=0.0 and val_data=None.\"\n",
        "                )\n",
        "\n",
        "        # finally fit the model\n",
        "        if validation_data is not None:\n",
        "            history = self.model.fit(\n",
        "                self.training_inputs,\n",
        "                self.training_outputs,\n",
        "                epochs=epochs,\n",
        "                validation_data=validation_data,\n",
        "                callbacks=callbacks,\n",
        "                **keras_kwargs,\n",
        "            )\n",
        "        else:\n",
        "            history = self.model.fit(\n",
        "                self.training_inputs,\n",
        "                self.training_outputs,\n",
        "                epochs=epochs,\n",
        "                validation_split=validation_split,\n",
        "                callbacks=callbacks,\n",
        "                **keras_kwargs,\n",
        "            )\n",
        "\n",
        "        return history\n",
        "\n",
        "    def predict(\n",
        "        self,\n",
        "        inputs: np.ndarray,\n",
        "        scale: Optional[bool] = None,\n",
        "        unscale_output: Optional[bool] = None,\n",
        "        **keras_kwargs: Optional[Any],\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Generates model predictions for given inputs, with optional scaling based on training settings.\n",
        "        :param inputs: The input data to be run through the model\n",
        "        :param scale: Whether the data should be scaled at the input. Scaling will ensure the model inputs are\n",
        "        scaled as set in set_training_data and `scale = None` will scale data if trained with scaling.\n",
        "        :param unscale_output: Whether the data should be unscaled at the output. Unscaling will ensure the model\n",
        "        outputs are scaled as set in set_training_data and `unscale = None` will unscale data if trained with unscaling.\n",
        "        :param keras_kwargs: parameters to be passed to the keras predict function if needed.\n",
        "        :return: The model predictions\n",
        "        \"\"\"\n",
        "\n",
        "        # Default input and output scaling to whatever was used in training\n",
        "        if scale is None:\n",
        "            scale = self._input_transform_args is not None\n",
        "\n",
        "        if unscale_output is None:\n",
        "            unscale_output = self._output_transform_args is not None\n",
        "\n",
        "        if scale:\n",
        "            inputs = self._transform(inputs, True)\n",
        "\n",
        "        # call the prediction\n",
        "        outputs = self.model.predict(inputs, **keras_kwargs)\n",
        "\n",
        "        # unscale if necessary\n",
        "        if unscale_output:\n",
        "            outputs = self._inverse_transform(outputs, is_input=False)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "# LINN format conversion\n",
        "\n",
        "def _eprint(*args, **kwargs):\n",
        "    print(*args, file=sys.stderr, **kwargs)\n",
        "\n",
        "\n",
        "custom_objects = {\"OutputClipLayer\": OutputClipLayer, \"WeightClip\": WeightClip}\n",
        "\n",
        "\n",
        "def convert_keras_to_linn(\n",
        "    model: keras.models.Model, input_channels: int, output_channels: int, **kwargs\n",
        "):\n",
        "    dims: List[int] = []\n",
        "    jlayers: List[dict] = []\n",
        "\n",
        "    assert isinstance(\n",
        "        model, (keras.Sequential, keras.Model, LinnModel)\n",
        "    ), f\"Unsupported model {type(model)}\"\n",
        "\n",
        "    if isinstance(model, LinnModel):\n",
        "        model = model.model\n",
        "\n",
        "    colDepth = 0\n",
        "    hardwareLayers = 0\n",
        "\n",
        "    for i, layer in enumerate(model.layers):\n",
        "        if isinstance(layer, (WeightClip, OutputClipLayer, keras.layers.InputLayer)):\n",
        "            _eprint(f\"Skipping layer {i} with type {type(layer)}\")\n",
        "            continue\n",
        "\n",
        "        if isinstance(layer, keras.layers.InputLayer):\n",
        "            assert i == 0, \"Input layer must be first layer\"\n",
        "            inputs = layer.batch_shape[0]\n",
        "            dims.append(inputs)\n",
        "            _eprint(f\"Input shape {inputs}\")\n",
        "        else:\n",
        "\n",
        "            assert isinstance(\n",
        "                layer, keras.layers.Dense\n",
        "            ), f\"Only Dense layers supported, got ({type(layer)})\"\n",
        "\n",
        "            layerWeights = layer.get_weights()\n",
        "            assert len(layerWeights) == 2, \"Layer must only contains weights and biases\"\n",
        "\n",
        "            # layers are stored in column-major order\n",
        "            weights = layerWeights[0].transpose()\n",
        "            bias = layerWeights[1]\n",
        "\n",
        "            assert len(weights.shape) == 2, \"Only two dimensional layers supported\"\n",
        "            assert len(bias.shape) == 1, \"Biases must be a vector\"\n",
        "\n",
        "            nRows = weights.shape[0]\n",
        "            nCols = weights.shape[1]\n",
        "            nBias = bias.shape[0]\n",
        "\n",
        "            assert 0 < nBias <= 100, \"Number of output rows must be 100 or fewer\"\n",
        "            assert nBias == nRows, \"Weights output does not match bias vector size\"\n",
        "            if hardwareLayers == 0:\n",
        "                # Handle the input to the network\n",
        "                assert 1 <= nCols <= 100, \"Maximum network input size is 100\"\n",
        "                dims.append(nCols)\n",
        "            else:\n",
        "                assert nCols == dims[-1], (\n",
        "                    f\"Input size {nCols} does not match output \"\n",
        "                    f\"of previous layer {dims[-1]}\"\n",
        "                )\n",
        "            dims.append(nRows)\n",
        "\n",
        "            # Ensure that the total number of weights and biases is not too large\n",
        "            # to fit into the memory for each neuron.\n",
        "            colDepth += nCols + 3  # bias\n",
        "            hardwareLayers += 1\n",
        "\n",
        "            jlayers.append(\n",
        "                {\n",
        "                    \"activation\": layer.activation.__name__.lower(),\n",
        "                    \"weights\": weights.tolist(),\n",
        "                    \"biases\": bias.tolist(),\n",
        "                }\n",
        "            )\n",
        "\n",
        "    assert hardwareLayers <= 5, \"Only five dense layers allowed\"\n",
        "\n",
        "    assert colDepth <= 1024, (\n",
        "        \"Total number of weights and biases too large,\"\n",
        "        \"sum(L[0].shape[1] for L in layers) + 3*len(layers)\"\n",
        "        f\" = {colDepth} must be <= 1024\"\n",
        "    )\n",
        "\n",
        "    _eprint(f\"Network latency approx. {colDepth} cycles\")\n",
        "\n",
        "    if \"output_mapping\" in kwargs:\n",
        "        output_map = kwargs.get(\"output_mapping\")\n",
        "        prev_final_weights = deepcopy(jlayers[-1][\"weights\"])\n",
        "        prev_final_bias = deepcopy(jlayers[-1][\"biases\"])\n",
        "        assert isinstance(output_map, list), \"Output mapping must be a list\"\n",
        "        assert len(output_map) <= len(prev_final_weights), (\n",
        "            f\"Output mapping must have less than {len(prev_final_weights)} elements\"\n",
        "        )\n",
        "        assert len(output_map) > 0, \"Output mapping must have at least one element\"\n",
        "\n",
        "        jlayers[-1][\"weights\"] = [[0] * len(prev_final_weights[0])] * len(output_map)\n",
        "        jlayers[-1][\"biases\"] = [0] * len(output_map)\n",
        "\n",
        "        for i, output_index in enumerate(output_map):\n",
        "            assert isinstance(output_index, int), \"Output mapping must be a list of integers\"\n",
        "            assert 0 <= output_index < len(prev_final_weights), \"Output mapping must be in range of output size\"\n",
        "            jlayers[-1][\"weights\"][i] = prev_final_weights[output_index]\n",
        "            jlayers[-1][\"biases\"][i] = prev_final_bias[output_index]\n",
        "\n",
        "    # Attempt to pretty print in a more readable form than json.dump; weights\n",
        "    # will print as a 2D grid instead of a linear list of lists\n",
        "    assert len(dims) > 1\n",
        "\n",
        "    if len(jlayers[0][\"weights\"][0]) > 4:\n",
        "        assert (\n",
        "            input_channels == 1\n",
        "        ), \"Only one channel is supported for > 4 network inputs\"\n",
        "    else:\n",
        "        assert (\n",
        "            input_channels == 1 or input_channels == len(jlayers[0][\"weights\"][0])\n",
        "        ), f\"Input channels must match input size {len(jlayers[0]['weights'][0])}\"\n",
        "\n",
        "    if len(jlayers[-1][\"weights\"]) > 4:\n",
        "        assert (\n",
        "            output_channels == 1\n",
        "        ), \"Only one channel is supported for > 4 network outputs\"\n",
        "    else:\n",
        "        assert (\n",
        "            output_channels == 1 or output_channels == len(jlayers[-1][\"weights\"])\n",
        "        ), f\"Output channels must match output size {len(jlayers[-1]['weights'])}\"\n",
        "\n",
        "    return {\n",
        "        \"version\": \"0.1\",\n",
        "        \"num_input_channels\": input_channels,\n",
        "        \"num_output_channels\": output_channels,\n",
        "        \"layers\": jlayers,\n",
        "    }\n",
        "\n",
        "\n",
        "def get_linn(\n",
        "    model: keras.models.Model, input_channels: int, output_channels: int, **kwargs\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Converts a 'LinnModel' into the '.linn' format required for execution on the\n",
        "    Moku Neural Network Instrument. This function will also work with compatible\n",
        "    Keras models if configured according to 'LinnModel' standards.\n",
        "\n",
        "    Args:\n",
        "        model (keras.models.Model): The 'LinnModel' instance or a compatible Keras model.\n",
        "        input_channels: An integer of the number of instrument inputs to connect to the network.\n",
        "                        Determines processing mode (serial or parallel) based on the ratio between\n",
        "                        'input_channels' and the number of input neurons in the model.\n",
        "        output_channels: An integer of the number of instrument outputs to connect to the network.\n",
        "                        Determines processing mode (serial or parallel) based on the ratio between\n",
        "                        'output_channels' and the number of output neurons in the model.\n",
        "    Keyword Args (Optional):\n",
        "        output_mapping (list): A list of integers that selects which output neurons\n",
        "                               should be used as the final output of the network.\n",
        "\n",
        "    Returns:\n",
        "        dict: The .linn JSON document or a dict of the network parameters suitable for loading in to the Neural Network instrument.\n",
        "    \"\"\"\n",
        "    return convert_keras_to_linn(\n",
        "        model=model, input_channels=input_channels, output_channels=output_channels, **kwargs\n",
        "    )\n",
        "\n",
        "\n",
        "def save_linn(\n",
        "    model: keras.models.Model, input_channels: int, output_channels: int, file_name: str, **kwargs\n",
        "):\n",
        "    \"\"\"\n",
        "    Converts a Keras model which is suitable for execution on the Moku\n",
        "    Neural Network Instrument into the `.linn` format and saves it to a\n",
        "    file.\n",
        "\n",
        "    Args:\n",
        "        model (keras.models.Model): The 'LinnModel' instance or a compatible Keras model.\n",
        "        input_channels: An integer of the number of instrument inputs to connect to the network.\n",
        "                        Determines processing mode (serial or parallel) based on the ratio between\n",
        "                        'input_channels' and the number of input neurons in the model.\n",
        "        output_channels: An integer of the number of instrument outputs to connect to the network.\n",
        "                        Determines processing mode (serial or parallel) based on the ratio between\n",
        "                        'output_channels' and the number of output neurons in the model.\n",
        "        file_name (str): Name of output .linn file, requires .linn extension.\n",
        "\n",
        "    Keyword Args (Optional):\n",
        "        output_mapping (list): A list of integers that selects which output neurons\n",
        "                               should be used as the final output of the network.\n",
        "    Returns:\n",
        "        None. Saves the result to a .linn file for loading in to the Neural Network instrument.\n",
        "    \"\"\"\n",
        "\n",
        "    linn_data = convert_keras_to_linn(\n",
        "        model=model, input_channels=input_channels, output_channels=output_channels, **kwargs\n",
        "    )\n",
        "    with open(file_name, \"w+\") as _writer:\n",
        "        json.dump(linn_data, _writer)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `load_linn(...)`\n",
        "\n",
        "This function:\n",
        "- Reads a `.linn` file.\n",
        "- Builds a `LinnModel` via `construct_model`.\n",
        "- Loads weights/biases.\n",
        "- Return compiled `LinnModel`."
      ],
      "metadata": {
        "id": "feVsynnk3hW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_linn(\n",
        "    file_name: str,\n",
        "    show_summary: bool = False,\n",
        "    optimizer: Any = \"adam\",\n",
        "    loss: Any = \"mse\",\n",
        "    metrics: Any = (),\n",
        ") -> LinnModel:\n",
        "    \"\"\"\n",
        "    Load a `.linn` file and return a LinnModel with an equivalent compiled Keras model.\n",
        "    Args:\n",
        "        file_name: Path to the .linn file.\n",
        "        show_summary: If True, print the model summary after construction.\n",
        "        optimizer, loss, metrics: Passed to construct_model() for compilation.\n",
        "    Returns:\n",
        "        LinnModel\n",
        "    Raises:\n",
        "        json.JSONDecodeError, KeyError, TypeError, ValueError on malformed inputs.\n",
        "    \"\"\"\n",
        "    # Read/parse JSON\n",
        "    with open(file_name, \"r\", encoding=\"utf-8\") as f:\n",
        "        try:\n",
        "            data = json.load(f)\n",
        "        except json.JSONDecodeError as e:\n",
        "            raise json.JSONDecodeError(f\"Invalid Linn file format: {e.msg}\", e.doc, e.pos) from e\n",
        "\n",
        "    # Extract required fields with Error checks\n",
        "    try:\n",
        "        input_channels = data[\"num_input_channels\"]\n",
        "    except KeyError:\n",
        "        raise KeyError(\"Missing attribute: num_input_channels\")\n",
        "    try:\n",
        "        output_channels = data[\"num_output_channels\"]\n",
        "    except KeyError:\n",
        "        raise KeyError(\"Missing attribute: num_output_channels\")\n",
        "    try:\n",
        "        jlayers = data[\"layers\"]\n",
        "    except KeyError:\n",
        "        raise KeyError(\"Missing attribute: layers\")\n",
        "    if not isinstance(jlayers, list) or len(jlayers) == 0:\n",
        "        raise ValueError(\"`layers` must be a non-empty list.\")\n",
        "\n",
        "    # Basic per-layer validation and dimension inference\n",
        "    for li, L in enumerate(jlayers):\n",
        "        for k in (\"activation\", \"weights\", \"biases\"):\n",
        "            if k not in L:\n",
        "                raise ValueError(f\"Layer {li} missing key '{k}'.\")\n",
        "        act = str(L[\"activation\"])\n",
        "        if act not in list_activations():\n",
        "            raise ValueError(\n",
        "                f\"Activation '{act}' (layer {li}) is not supported. Try {list_activations()}.\"\n",
        "            )\n",
        "        W = L[\"weights\"]\n",
        "        b = L[\"biases\"]\n",
        "        if not isinstance(W, list) or len(W) == 0 or not all(isinstance(r, list) for r in W):\n",
        "            raise ValueError(f\"Layer {li}: 'weights' must be a non-empty 2D list.\")\n",
        "        row_len = len(W[0])\n",
        "        if row_len == 0 or any(len(r) != row_len for r in W):\n",
        "            raise ValueError(f\"Layer {li}: all weight rows must have the same non-zero length.\")\n",
        "        if not isinstance(b, list) or len(b) == 0:\n",
        "            raise ValueError(f\"Layer {li}: 'biases' must be a non-empty 1D list.\")\n",
        "        if len(b) != len(W):\n",
        "            raise ValueError(f\"Layer {li}: len(biases) must equal number of units (rows in weights).\")\n",
        "\n",
        "    input_dim = len(jlayers[0][\"weights\"][0])   # cols of first layer\n",
        "    output_dim = len(jlayers[-1][\"weights\"])    # rows of last layer\n",
        "\n",
        "    # Channel constraints\n",
        "    if len(jlayers[0][\"weights\"][0]) > 4:\n",
        "        if input_channels != 1:\n",
        "            raise ValueError(\"Only one channel is supported for > 4 network inputs\")\n",
        "    else:\n",
        "        if input_channels not in (1, len(jlayers[0][\"weights\"][0])):\n",
        "            raise ValueError(f\"Input channels must match input size {len(jlayers[0]['weights'][0])}\")\n",
        "\n",
        "    if len(jlayers[-1][\"weights\"]) > 4:\n",
        "        if output_channels != 1:\n",
        "            raise ValueError(\"Only one channel is supported for > 4 network outputs\")\n",
        "    else:\n",
        "        if output_channels not in (1, len(jlayers[-1][\"weights\"])):\n",
        "            raise ValueError(f\"Output channels must match output size {len(jlayers[-1]['weights'])}\")\n",
        "\n",
        "    # Build layer definition for construct_model\n",
        "    layer_definitions: List[Tuple[int, str]] = [(len(L[\"weights\"]), str(L[\"activation\"])) for L in jlayers]\n",
        "\n",
        "    # Bootstrap LinnModel with dummy shapes so construct_model knows dims\n",
        "    lm = LinnModel()\n",
        "    lm.set_training_data(\n",
        "        training_inputs=np.zeros((1, input_dim), dtype=np.float32),\n",
        "        training_outputs=np.zeros((1, output_dim), dtype=np.float32),\n",
        "        scale=False,\n",
        "    )\n",
        "    lm.construct_model(\n",
        "        layer_definition=layer_definitions,\n",
        "        show_summary=show_summary,\n",
        "        optimizer=optimizer,\n",
        "        loss=loss,\n",
        "        metrics=metrics,\n",
        "    )\n",
        "\n",
        "    # Load weights (transpose from LINN to Keras)\n",
        "    dense_idx = 0\n",
        "    for layer in lm.model.layers:\n",
        "        if isinstance(layer, keras.layers.Dense):\n",
        "            W_json = np.asarray(jlayers[dense_idx][\"weights\"], dtype=np.float32)  # (units, in_dim)\n",
        "            b_json = np.asarray(jlayers[dense_idx][\"biases\"], dtype=np.float32)   # (units,)\n",
        "\n",
        "            kshape = layer.kernel.shape  # (in_dim, units)\n",
        "            in_dim_expected = int(kshape[0])\n",
        "            units_expected = int(kshape[1])\n",
        "\n",
        "            if W_json.shape != (units_expected, in_dim_expected):\n",
        "                raise ValueError(\n",
        "                    f\"Layer {dense_idx}: weight matrix shape mismatch; \"\n",
        "                    f\"expected (units, in_dim)=({units_expected},{in_dim_expected}), got {tuple(W_json.shape)}.\"\n",
        "                )\n",
        "            if b_json.shape != (units_expected,):\n",
        "                raise ValueError(\n",
        "                    f\"Layer {dense_idx}: bias shape mismatch; expected ({units_expected},), got {tuple(b_json.shape)}.\"\n",
        "                )\n",
        "\n",
        "            layer.set_weights([W_json.T, b_json])\n",
        "            dense_idx += 1\n",
        "\n",
        "    _LOG.info(\n",
        "        \"Loaded LINN model from %s (in=%d, out=%d, layers=%d, chan_in=%d, chan_out=%d)\",\n",
        "        file_name, input_dim, output_dim, len(jlayers), input_channels, output_channels\n",
        "    )\n",
        "    return lm"
      ],
      "metadata": {
        "id": "eLHFdmMTzXbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example: Load a `.linn` model and fine-tune with additional data\n",
        "\n",
        "This example:\n",
        "- Loads a pre-trained model from a `.linn` file.\n",
        "- Creates some additional synthetic training pairs.\n",
        "- Fine-tunes the model with the additional data."
      ],
      "metadata": {
        "id": "LklG_HOr3HGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "LINN_PATH = \"example.linn\" # Change this to your \".linn\" model path\n",
        "\n",
        "# Load the model\n",
        "lm = load_linn(LINN_PATH, show_summary=True, optimizer=\"adam\", loss=\"mse\", metrics=(\"mae\",)) # These parameters can be changed and are passed on to construct_model().\n",
        "\n",
        "# Prepare additional data (demo: synthetic data).\n",
        "input_size = lm.model.input_shape[-1]\n",
        "output_size = lm.model.output_shape[-1]\n",
        "\n",
        "# We create synthetic feature data for this tutorial.\n",
        "# For an acutal use case, replace X_extra with real data features.\n",
        "X_extra = np.random.uniform(-1, 1, size=(512, input_size))\n",
        "\n",
        "# For this tutorial, we generate our synthetic label data by adding noise to the predictions of our previous model.\n",
        "# For an actual use case, replace Y_extra real prediction labels.\n",
        "Y_pred = lm.model.predict(X_extra, verbose=0)\n",
        "Y_extra = (Y_pred + np.random.normal(0, 0.05, size=Y_pred.shape))\n",
        "\n",
        "# Fine-tune\n",
        "history = lm.model.fit(\n",
        "    X_extra, Y_extra,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    validation_split=0.1,\n",
        "    verbose=1,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "z76UiXz9z_d_",
        "outputId": "a98fe613-c0e3-4170-d922-2cc522cd0663"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_5\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_5\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m\n",
              "\n",
              " input_layer_5 (\u001b[38;5;33mInputLayer\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                          \u001b[38;5;34m0\u001b[0m \n",
              "\n",
              " dense_20 (\u001b[38;5;33mDense\u001b[0m)                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)                        \u001b[38;5;34m528\u001b[0m \n",
              "\n",
              " output_clip_layer_20             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)                          \u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mOutputClipLayer\u001b[0m)                                                      \n",
              "\n",
              " dense_21 (\u001b[38;5;33mDense\u001b[0m)                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                          \u001b[38;5;34m34\u001b[0m \n",
              "\n",
              " output_clip_layer_21             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                           \u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mOutputClipLayer\u001b[0m)                                                      \n",
              "\n",
              " dense_22 (\u001b[38;5;33mDense\u001b[0m)                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)                         \u001b[38;5;34m48\u001b[0m \n",
              "\n",
              " output_clip_layer_22             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)                          \u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mOutputClipLayer\u001b[0m)                                                      \n",
              "\n",
              " dense_23 (\u001b[38;5;33mDense\u001b[0m)                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                          \u001b[38;5;34m17\u001b[0m \n",
              "\n",
              " output_clip_layer_23             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                           \u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mOutputClipLayer\u001b[0m)                                                      \n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\"> Layer (type)                    </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">       Param # </span>\n",
              "\n",
              " input_layer_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              "\n",
              " dense_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                        <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> \n",
              "\n",
              " output_clip_layer_20             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">OutputClipLayer</span>)                                                      \n",
              "\n",
              " dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span> \n",
              "\n",
              " output_clip_layer_21             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">OutputClipLayer</span>)                                                      \n",
              "\n",
              " dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span> \n",
              "\n",
              " output_clip_layer_22             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">OutputClipLayer</span>)                                                      \n",
              "\n",
              " dense_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> \n",
              "\n",
              " output_clip_layer_23             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">OutputClipLayer</span>)                                                      \n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m627\u001b[0m (2.45 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">627</span> (2.45 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m627\u001b[0m (2.45 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">627</span> (2.45 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 67ms/step - loss: 0.3577 - mae: 0.5094 - val_loss: 0.3587 - val_mae: 0.5148\n",
            "Epoch 2/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.3414 - mae: 0.5005 - val_loss: 0.3552 - val_mae: 0.5136\n",
            "Epoch 3/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3305 - mae: 0.4934 - val_loss: 0.3510 - val_mae: 0.5113\n",
            "Epoch 4/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3305 - mae: 0.4933 - val_loss: 0.3478 - val_mae: 0.5096\n",
            "Epoch 5/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3308 - mae: 0.4940 - val_loss: 0.3465 - val_mae: 0.5094\n",
            "Epoch 6/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3234 - mae: 0.4883 - val_loss: 0.3453 - val_mae: 0.5089\n",
            "Epoch 7/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3272 - mae: 0.4931 - val_loss: 0.3443 - val_mae: 0.5086\n",
            "Epoch 8/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3249 - mae: 0.4903 - val_loss: 0.3433 - val_mae: 0.5082\n",
            "Epoch 9/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3227 - mae: 0.4890 - val_loss: 0.3424 - val_mae: 0.5078\n",
            "Epoch 10/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3236 - mae: 0.4898 - val_loss: 0.3419 - val_mae: 0.5077\n",
            "Epoch 11/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3219 - mae: 0.4883 - val_loss: 0.3413 - val_mae: 0.5076\n",
            "Epoch 12/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3256 - mae: 0.4923 - val_loss: 0.3409 - val_mae: 0.5073\n",
            "Epoch 13/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3239 - mae: 0.4904 - val_loss: 0.3404 - val_mae: 0.5072\n",
            "Epoch 14/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3236 - mae: 0.4908 - val_loss: 0.3401 - val_mae: 0.5072\n",
            "Epoch 15/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3272 - mae: 0.4942 - val_loss: 0.3399 - val_mae: 0.5071\n",
            "Epoch 16/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3238 - mae: 0.4902 - val_loss: 0.3396 - val_mae: 0.5070\n",
            "Epoch 17/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3277 - mae: 0.4932 - val_loss: 0.3393 - val_mae: 0.5069\n",
            "Epoch 18/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3225 - mae: 0.4899 - val_loss: 0.3391 - val_mae: 0.5068\n",
            "Epoch 19/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3200 - mae: 0.4870 - val_loss: 0.3389 - val_mae: 0.5066\n",
            "Epoch 20/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3195 - mae: 0.4868 - val_loss: 0.3387 - val_mae: 0.5067\n",
            "Epoch 21/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3171 - mae: 0.4856 - val_loss: 0.3386 - val_mae: 0.5066\n",
            "Epoch 22/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3224 - mae: 0.4898 - val_loss: 0.3385 - val_mae: 0.5065\n",
            "Epoch 23/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3190 - mae: 0.4868 - val_loss: 0.3384 - val_mae: 0.5065\n",
            "Epoch 24/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3240 - mae: 0.4900 - val_loss: 0.3382 - val_mae: 0.5065\n",
            "Epoch 25/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3216 - mae: 0.4880 - val_loss: 0.3381 - val_mae: 0.5064\n",
            "Epoch 26/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3261 - mae: 0.4922 - val_loss: 0.3381 - val_mae: 0.5063\n",
            "Epoch 27/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3238 - mae: 0.4905 - val_loss: 0.3380 - val_mae: 0.5063\n",
            "Epoch 28/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3224 - mae: 0.4900 - val_loss: 0.3380 - val_mae: 0.5063\n",
            "Epoch 29/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3228 - mae: 0.4895 - val_loss: 0.3379 - val_mae: 0.5063\n",
            "Epoch 30/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3220 - mae: 0.4898 - val_loss: 0.3378 - val_mae: 0.5063\n",
            "Epoch 31/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3225 - mae: 0.4894 - val_loss: 0.3377 - val_mae: 0.5062\n",
            "Epoch 32/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3254 - mae: 0.4929 - val_loss: 0.3377 - val_mae: 0.5062\n",
            "Epoch 33/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3204 - mae: 0.4877 - val_loss: 0.3377 - val_mae: 0.5061\n",
            "Epoch 34/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3277 - mae: 0.4945 - val_loss: 0.3376 - val_mae: 0.5061\n",
            "Epoch 35/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3218 - mae: 0.4897 - val_loss: 0.3376 - val_mae: 0.5061\n",
            "Epoch 36/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3233 - mae: 0.4909 - val_loss: 0.3375 - val_mae: 0.5061\n",
            "Epoch 37/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3207 - mae: 0.4879 - val_loss: 0.3376 - val_mae: 0.5061\n",
            "Epoch 38/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3174 - mae: 0.4851 - val_loss: 0.3374 - val_mae: 0.5060\n",
            "Epoch 39/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3224 - mae: 0.4897 - val_loss: 0.3374 - val_mae: 0.5060\n",
            "Epoch 40/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3237 - mae: 0.4907 - val_loss: 0.3374 - val_mae: 0.5060\n",
            "Epoch 41/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3231 - mae: 0.4902 - val_loss: 0.3374 - val_mae: 0.5060\n",
            "Epoch 42/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3231 - mae: 0.4908 - val_loss: 0.3373 - val_mae: 0.5060\n",
            "Epoch 43/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3215 - mae: 0.4878 - val_loss: 0.3373 - val_mae: 0.5059\n",
            "Epoch 44/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3192 - mae: 0.4851 - val_loss: 0.3373 - val_mae: 0.5060\n",
            "Epoch 45/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3194 - mae: 0.4865 - val_loss: 0.3373 - val_mae: 0.5059\n",
            "Epoch 46/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3193 - mae: 0.4869 - val_loss: 0.3372 - val_mae: 0.5059\n",
            "Epoch 47/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.3219 - mae: 0.4900 - val_loss: 0.3372 - val_mae: 0.5059\n",
            "Epoch 48/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3216 - mae: 0.4894 - val_loss: 0.3372 - val_mae: 0.5059\n",
            "Epoch 49/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3201 - mae: 0.4871 - val_loss: 0.3371 - val_mae: 0.5059\n",
            "Epoch 50/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3235 - mae: 0.4913 - val_loss: 0.3372 - val_mae: 0.5058\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Notes\n",
        "\n",
        "- To export the fine-tuned model back to `.linn`, use `save_linn(...)`.\n",
        "- For real tasks, replace the synthetic `X_extra, Y_extra` with properly labeled data."
      ],
      "metadata": {
        "id": "NJT37wCc3wGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out_idx = lm.model.output_shape[-1] - 1  # last output neuron index\n",
        "save_linn(lm, input_channels=1, output_channels=1, output_mapping=[out_idx], file_name='example_Retrained.linn')\n",
        "\n",
        "# If we want all outputs exported, we can omit the output_mapping parameter entirely.\n",
        "# output_mapping tells save_linn exactly which output neuron indices from the final layer to include in the exported network. This is especially useful when:\n",
        "      # You trained with sliding windows and want to export only a subset (often the final time step) to avoid overlapping the same target sample.\n",
        "      # You want to reduce exported outputs to meet channel constraints or simplify wiring."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4-q2fXKSpRW",
        "outputId": "42e7cdfb-c020-4353-8012-3350222dad10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Skipping layer 0 with type <class 'keras.src.layers.core.input_layer.InputLayer'>\n",
            "Skipping layer 2 with type <class '__main__.OutputClipLayer'>\n",
            "Skipping layer 4 with type <class '__main__.OutputClipLayer'>\n",
            "Skipping layer 6 with type <class '__main__.OutputClipLayer'>\n",
            "Skipping layer 8 with type <class '__main__.OutputClipLayer'>\n",
            "Network latency approx. 78 cycles\n"
          ]
        }
      ]
    }
  ]
}